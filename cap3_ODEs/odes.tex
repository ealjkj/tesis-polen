%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 1%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Ecuaciones diferenciales}
\label{odes_chapter}
Las ResNets y las Ecuaciones Diferenciales Ordinarias guardan cierta relación que será estudiada a detalle en el Capítulo \ref{odes_and_resnets}. En este capítulo se presenta una introducción a las Ecuaciones Diferenciales Ordinarias (ODEs por sus siglas en inglés) y a los métodos numéricos necesarios para su resolusión. Para un estudio más extensivo se puede consultar \cite{sauer, Ascher}.

Desde su implementación en el sigo XVII \cite{ode_history} las ecuaciones diferenciales han sido una herramienta de mucha utilidad para las ciencias, con aplicaciones en geometría \cite{geometry_and_odes}, mecánica \cite{classical_mechanics}, electromagnetismo \cite{electromagnetism_and_odes}, etc.

Una ecuación diferencial ordinaria se representa de la siguiente manera:
\begin{equation}
    \frac{dx}{dt} = f(x(t), t),
\end{equation}
donde $f: \mathbb R^m \to \mathbb R^n$.
En caso de que no exista ambiguedad, es posible escribir $x$ en lugar de $x(t)$ y $x'$ para referirse a la derivada de $x$ con respecto a $t$, con lo que la ecuación anterior se puede escribir de la siguiente manera:
\begin{equation}
    x' = f(x,t)
\end{equation}
En este caso, ya que la variable dependiente es el tiempo $t$, la derivada con respecto al tiempo se puede escribir con un punto, es decir: $\dot x = \frac{dx}{dt}$. En este trabajo, la variable dependiente será el tiempo, pero la teoría se puede extender para distintos dominios.
\section{Problemas de valor inicial}
Considérese el siguiente problema de valor inicial (IVP por sus siglas en inglés)
\begin{align}
    \label{ode} \dot x(t) = f(x, t),& \quad t>0\\
    \label{initial_value} x(t_0) = \nu,&
\end{align}
donde $t\in [t_0, t_f]$. 

\subsection{Existencia, unicidad y continuidad de soluciones}
\begin{theorem}
    Sea $f: [a_1,b_1] \times [a_2, b_2]$ una una función Lipschitz continua en la segunda entrada y $a_2 < x_a < b_2$. Entonces existe un $c\in [a_1, b_1]$ tal que el IVP
    \begin{equation}
        \left\{\quad \begin{matrix}
            \dot x = f(x,t) \\
            x(a_1) = x_a \\
            t \in [a_1,c]
        \end{matrix}\right.
    \end{equation} 
    tiene exactamente una solución $x(t)$. Aún más, si $f$ es Lipschitz continua en $[a_1, b_1]\times (-\infty, \infty)$, entonces existe exactamente una solución en $[a_1, b_1]$. 
\end{theorem}
La demostración se puede encontrar en \cite{sauer}

\section{Estabilidad}
\label{ode_stability}
En la literatura, no existe una convención al hablar de estabilidad de ecuaciones diferenciales \cite{richard-Bellman, Hyers-Ulam}, sin embargo adoptaremos las definiciones de Ascher \cite{ascher-book}.

Cuando se hacen cambios mínimos en el valor inicial del IVP (\ref{ode} - \ref{initial_value}), uno se pregunta ¿qué ocurre con la solución? ¿Los cambios también serán pequeños? Para el análisis de estabilidad esto, se requieren algunas definiciones

\begin{definition}
    Una solución $x(t)$ es llamada \textbf{estable} si para todo $\varepsilon > 0$ existe un $\delta > 0$ tal que cualquier otra solución $\hat x(t)$ de \ref{ode} que satisfaga 
    \begin{equation}
        |x(t_0)  - \hat x(t_0)| \leq \delta
    \end{equation}
    también satisface 
    \begin{equation}
        \label{stable-ode}
        |x(t)- \hat x(t)| \leq \varepsilon, \quad \forall t > t_0.
    \end{equation} 
    La solución $x(t)$ es \textbf{asintónticamente estable} si en adición a (\ref{stable-ode}) se cumple que 
    \begin{equation}
        |x(t)- \hat x(t)| \to 0, \quad si \quad t\to \infty.
    \end{equation}
    Por otro lado, $x(t)$ es \textbf{relativamente estable} si en vez de (\ref{stable-ode}) se satisface que 
    \begin{equation}
        |x(t)- \hat x(t)| \leq \varepsilon|x(t)|, \quad \forall t > t_0.
    \end{equation}
\end{definition}

\begin{definition}
    Una solución es llamada \textbf{uniformemente estable} si, para cada $\varepsilon > 0$, existe $\delta > 0$ tal que cualquier otra solución $\hat x(t)$ de (\ref{ode}) que satisfaga
    \begin{equation}
        |x(c) - \hat x(c)| \leq \delta
    \end{equation}
    para algún punto $c \geq t_0$, también se satisface que
    \begin{equation}
        |x(t) - \hat x(t)| \leq \varepsilon, \quad \forall t > c.
    \end{equation}
\end{definition}

La \textbf{estabilidad asintótica uniforme} y la \textbf{estabilidad relativa uniforme} se definen de manera análoga. Es evidente que la estabilidad uniforme, implica estabilidad.

\subsection{Problema Lineal}
Considérese el IVP homogéneo 
    \begin{equation}
        \label{hom_ivp}
        \dot x = A(t)x, \quad  t > t_0
    \end{equation}
    \begin{equation}
        \label{hom_init}
        x(t_0) = \alpha
    \end{equation}


\begin{theorem}
    La solución $y$ IVP (\ref{hom_ivp}- \ref{hom_init}) con $A$ constante es uniformemente estable si y sólo si todos los eigenvalores de $A$ statisfacen que $\text{Re}(\lambda) < 0$ o si $\text{Re}(\lambda) = 0$ con $\lambda$ simple. 
\end{theorem}

Sin embargo, $A(t)$ no tiene por qué ser constante. Para un caso más general, se tiene el siguiente teorema.
\begin{theorem}
    \label{lineal_ivp}
    La solución $y$ del IVP (\ref{hom_ivp}- \ref{hom_init}) es uniformemente estable si se cumplen las siguentes dos condiciones
    \begin{enumerate}
        \item La matriz $A$ es \textbf{estrictamente diagonalmente dominante}, es decir 
        \begin{equation}
            \sum_{j\neq i} |a_{i,j}| \leq (1-\delta)|a_{ii}|, \quad i = 1, \cdots, n
        \end{equation}
        \item Los elementos de la diagonal son menores que 0 
        \begin{equation}
            Re(a_{ii}) < 0, \quad i = 1, \cdots n
        \end{equation}
    \end{enumerate}
\end{theorem}

Además, como consecuencia del Teorema de Gershgorin se tiene que si se satisfacen las hipótesis del Teorema (\ref{lineal_ivp}) los eigenvalores cumplen que $Re(\lambda_i) < 0$. De modo que los eigenvalores de nuestra matriz $A$ son elementos suficientes para determinar la estabilidad de una ecuación diferencial.

\subsection{IVP no lineales (El caso general)} 
\label{no_lineal_ivp}
Para determinar la estabilidad de un IVP no lineal, se puede hacer uso de los resultados obtenidos en la Subsección anterior. El tema de interés en esta ocasión es la ecuación 
\begin{equation}
    \label{original_ode}
    \dot x = f(x,t)
\end{equation}
\begin{equation}
    x(t_0) = \alpha
\end{equation}
Sea $\hat x$ una solución de (\ref{original_ode}), con $\hat x(t_0)$ no muy lejano de $x(t_0)$. Expandiendo $f(x,t)$ utilizando su serie de Taylor, se obtiene que 
\begin{equation}
    \label{taylor_for_ivp}
    f(\hat x, t) = f(y,t) + J(x,t)(\hat x - x) + r(x,\hat x, t),
\end{equation}
donde $J$ es el jacobiano definido por
\begin{equation}
    J(x, t) := \frac{\partial f}{ \partial x}.
\end{equation}
y el término $r$ es el residuo. Si se ignora el residuo $r$ en (\ref{taylor_for_ivp}), y tomando $z = x - \hat x$ se obtiene
\begin{equation}
    \dot z = J(t)z
\end{equation}
Por lo que para determinar la estabilidad, es suficiente conocer los eigenvalores del Jacobiano $J$.

\section{Métodos de Runge-Kutta}
Es posible generalizar el método de Euler, evaluando la derivada multiples veces en un paso. Esta idea se le atribuye a Runge \cite{book:110336}. y en 1901 Kutta hizo contribuciones importantes al método, dando paso a métodos de orden 4 y el primer método de orden 5, denominados métodos de Runge-Kutta. A continuación se describen alguno de los métodos de Runge-Kutta más relevantes.
\subsection{Método de Euler}
\label{section:euler_method}
La manera más elemental para resolver el IVP (\ref{ode} - \ref{initial_value}) es con el método de Euler. 

Nótese que es posible particionar el intervalo $[t_0, t_f]$ en $N$ partes iguales, obteniendo así la sucesión 
$$t_0, t_0 + h, t_0 + 2h, ... , t_0 + Nh,$$
donde
$$h = \frac{t_f - t_0}{N}.$$
Se denotará $t_n = t_0 + nh$ y  $x_n = x(t_n)$. De modo que se tiene la siguiente sucesión:
\begin{equation}
    t_0 < t_1 < t_2 < \cdots < t_N = t_f.
\end{equation}
El siguiente paso, es aproximar los valores de $x_n$, y a estas aproximaciones les llamaremos $w_n$, teniendo en cuenta que $w_0 = x_0$. Para ello, se hará uso de la expansión de Taylor de $x(t + h)$.
\begin{equation}
    \label{taylor}
    x(t + h) = x(t) + h\dot x(t) + R_1(t),
\end{equation}
donde $R_1(t)$ es el residuo de la expansión de Taylor. Sustituyendo $\dot x(t) = f(t, x(t))$, obtenemos 
\begin{equation}
    x(t+h) = x(t) + hf(t,x(t)) + R_1(t).
\end{equation}
Sustituyendo $t = t_n$ con $i < N$ obtenemos que
\begin{equation}
    x(t_n + h) = x(t_n) + hf(t_n, x(t_n)) + R_1(t_n).
\end{equation}
Por lo tanto, substrayendo $R_1(t)$, es posible obtener una sucesión que aproxime a la solución $x(t)$:
\begin{equation}
    w_{n+1} = w_n + hf(t_n, w_n), \quad w_0 = x_0.
\end{equation} 

\subsubsection{Error de truncamiento local}
En general, el error de truncamiento local de un método numérico, es el error generado a partir de una iteración. Es decir:
\begin{equation}
    e_n = |w_n - z(t_n)|
\end{equation}
donde $z$ es la solución del problema 
\begin{equation}
    \left\{
    \begin{matrix}
        \dot x = f(x,t) \\
        x(t_n) = w_n \\
        t \in [t_n, t_{n+1}]
    \end{matrix}    
    \right.
\end{equation}

En el caso del método de Euler, es posible encontrar el error de truncamiento local. Asumiendo que en la identidad (\ref{taylor}), $x(t)$ es doblemente diferenciable en el intervalo $(t_0, t_f)$, ocurre que
\begin{equation}
    R_1(t) = \frac{1}{2!} h^2\dot x(\xi),  \quad \xi\in (t, t+h).
\end{equation}
\subsubsection{Error de truncamiento global}
El error de truncamiento global de un método numérico, es el error generado por multiples iteraciones. 
\begin{equation}
    g_n = |w_n - x_n|
\end{equation}

Para investigar el error global del método de Euler, asúmase una cota superior $mh^2$ para la norma del error de truncamiento local, es decir $e_n \leq mh^2$. Además se asume Lipschitz continuidad para $f$, cuya constante de Lipschitz es $L$.

Es posible acotar el error global, de la siguiente manera:
\begin{equation}
    \label{bound_global_error}
    g_n \leq \frac{Ch^k}{L}(e^{L(t_n-a)}-1)
\end{equation}
La demostración de (\ref{bound_global_error}) puede encontrarse en \cite{sauer}. En general, se comparan los distintos métodos numéricos basándose en el error, y los mejores métodos son aquellos cuyo \textbf{orden} es mayor:
\begin{definition}
    Se dice que un método es de orden $\rho$ cuando $|w_n - x_n| = \mathcal O(h^{\rho})$.
\end{definition}

\subsection{Método de Euler modificado}
El siguiente método, es un ejemplo de un método de Runge-Kuta, el cuál se usará para desarrollar las ideas tras los métodos generales.
\begin{align*}
    k_1 &= f(t_n, w_n),\\
    k_2 &= f(t_n + \tfrac{1}{2}h,w_n + \tfrac{1}{2}hk_1),\\
    w_{n+1} &= w_n + hk_2,\\
    t_{n+1} &= t_{n}+h.
\end{align*}
Este es un método de segundo orden. 
\subsection{Método general}
El método de Runge-Kutta general con $s$ etapas se puede definir usando $s^2 + 2s$ números
\begin{equation}
    a_{i,j}, \quad i,j = 1,2,..., s. \qquad b_i, c_i, \quad i= 1, 2, ..., s,
\end{equation}
usando el siguiente esquema

\begin{equation}
    w_{n+1} = w_n + h\sum_{i=1}^sb_ik_i.
\end{equation}
En donde la sucesión $\{k_i\}$ es calculada usando la función $f$:
\begin{equation}
    k_i = f\left(t_n + c_ih, w_n + h\sum_{i=1}^sa_{i,j}k_i\right), \quad i = 1, 2, ..., s.
\end{equation}
Es posible determinar si el método es explícito o implícito, basándose en la matriz $A = \{a_{i,j}\}$. En caso de que sea una matriz triangular inferior, con todos los elementos de la diagonal iguales a cero ($a_{i,j} = 0$ para $j = i, i+1, ...,s$) el esquema es explícito.


Las siguientes relaciones, son conocidas como condiciones de orden
\begin{equation}
    \sum_{i=1}^s b_i=1, \quad \sum_{i,j=1}^sb_ia_{i,j}=\frac{1}{2}, \quad \sum_{i,j,k=1}^sb_i a_{i,j} a_{j,k}=\frac{1}{6}, \quad  \sum_{i,j,k=1}^sb_ia_{i,j}a_{i,k}=\frac{1}{3}
\end{equation}
y aseguran esquemas de al menos orden 3.

\subsection{Runge Kutta de orden 4 (RK4)}
Uno de los métodos más conocidos es el de orden 4
\begin{equation}
    w_{n+1} = w_n + \frac{h}{6}(s_1 + 2s_2 + 2s_3 + s_4),
\end{equation}
donde 
\begin{align*}
    s_1 &= f(t_n, w_n) \\
    s_2 &= f\left(t_n +\frac{h}{2}, w_n+\frac{h}{2}s_1\right)\\
    s_3 &= f\left(t_n +\frac{h}{2}, w_n+\frac{h}{2}s_2\right)\\
    s_4 &= f\left(t_n + h, w_n+hs_3\right).
\end{align*}
La simplicidad de este método, hace que sea muy fácil de programar, y al ser de orden 4, se prefiere por sobre otos métodos como Euler, o el Trapezoide. Como se puede apreciar, RK4 es un método de orden 4 y además tiene 4 etapas (i.e. $s=4$). Sin embargo, no se conoce cuántas etapas son necesarias para esquemas de órdenes mayores a 8. 
\subsection{Métodos de Runge Kutta simplécticos}
Dentro de la categoría de métodos de Runge Kutta, existen los métodos simplécticos \cite{symplecticRK}. Se caracterizan por preservar \textsl{invariantes cuadráticas} \cite{Sanz-Serna1988-mk, Lasagni1988-ov}.
\begin{definition}
    Un esquema de Runge Kutta se dice simpléctico si satisface la relación 
    \begin{equation}
        b_ia_{i,j} + b_ja_{j,i} - b_ib_j = 0, \quad i,j=1, ..., s.
    \end{equation}
\end{definition}
\subsection{Métodos implícitos}
Los métodos numéricos analizados hasta este momento se conocen como métodos explícitos, debido a que es posible calcular $w_{n+1}$ conociendo el valor $w_n$. 
\subsubsection{Euler hacia atrás}
Una versión implícita de Euler se conoce como Euler hacia atrás. 
\begin{align}
    w_0  &= x_0 \\
    \label{backward_euler} w_{n+1} &= w_n + hf(t_{n+1}, w_{n+1})
\end{align}
Difiere del método de Euler tradicional en que la pendiente que se usa involucra $w_{n+1}$. De modo que en cada nueva iteración, es necesario resolver un sistema de ecuaciones, y por consiguiente el costo computacional se incrementa.
