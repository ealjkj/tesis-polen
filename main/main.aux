\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Preliminares}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Resultados de Cálculo y Álgebra lineal}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces En las redes neuronales convolucionales, las imágenes pasan por múltiples filtros. El resultado de cada convolución, es una característica.}}{3}{figure.1.1}\protected@file@percent }
\newlabel{doubly_circulant_matrix}{{1.10}{3}{}{theorem.1.10}{}}
\citation{computer_vision}
\citation{Mitchell}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Visión Computacional}{5}{section.1.2}\protected@file@percent }
\newlabel{digital_image}{{1.17}{5}{}{theorem.1.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Inteligencia artificial y Aprendizaje profundo}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Sobreajusto y Desajuste}{6}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Cuando un modelo tiene sobreajuste se dice que memorizó las etiquetas de los datos, en vez de encontrar patrones para generalizar.}}{7}{figure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Cuando el error de entrenamiento es muy grande, la causa podría ser un desajuste.}}{8}{figure.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Regularización}{8}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Función de activación. }{9}{subsection.1.3.3}\protected@file@percent }
\citation{transfer}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Funciones de activación comunmente utilizadas}}{10}{figure.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Transferencia de aprendizaje}{10}{subsection.1.3.4}\protected@file@percent }
\citation{grafica}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Conjuntos desbalanceados}{11}{subsection.1.3.5}\protected@file@percent }
\newlabel{unbalanced_sets}{{1.3.5}{11}{Conjuntos desbalanceados}{subsection.1.3.5}{}}
\newlabel{balanced}{{1.19}{11}{}{theorem.1.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Ejemplos de conjuntos balanceados y desbalanceados.}}{12}{figure.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Afrontar un problema con un conjunto desbalanceado}{12}{section*.3}\protected@file@percent }
\citation{VAE}
\citation{SMOTE}
\citation{MSMOTE}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Representación gráfica del submuestreo y el sobremuestreo.}}{13}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Métricas distintas a la exactitud}{13}{section*.4}\protected@file@percent }
\newlabel{diff_metrics}{{1.21}{14}{}{theorem.1.21}{}}
\citation{firstCnn}
\citation{lecunCnn}
\citation{alexnet}
\citation{CNNdefinition,deeplearningbook}
\citation{convolution_for_quaternion}
\citation{convolution_invariance}
\citation{convolution_invariance}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Redes Neuronales Convolucionales}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convoluciones}{15}{section.2.1}\protected@file@percent }
\newlabel{convolution_definition}{{2.1}{15}{Convolución}{theorem.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representación gráfica de una convolución. Para cada posición del Kernel sobre la imagen, se realiza una multiplicación \textsl  {entrada por entrada} del Kernel y la un conjunto de pixeles de la imagen, del mismo tamaño que el Kernel.}}{17}{figure.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Stride}{17}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolución con tamaño de paso 2}}{18}{figure.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Convoluciones con múltiples canales}{18}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces La convolución 2D puede ser usada con características de $d_1$ canales y devolver una característica de $d_2$ canales.}}{19}{figure.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Padding}{19}{subsection.2.1.3}\protected@file@percent }
\citation{padding}
\citation{type_of_paddings}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \leavevmode {\color  {red}Usar una imagen propia}}}{20}{figure.2.4}\protected@file@percent }
\citation{type_of_paddings}
\citation{pooling_analysis}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Agrupación (Pooling)}{21}{subsection.2.1.4}\protected@file@percent }
\citation{CNNdefinition}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \textsc  {Ejemplo de maxpooling y averagepooling con regiones rectangulares de $2\times 2$.}}}{22}{figure.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\leavevmode {\color  {red}Global pooling layers} }{22}{section*.5}\protected@file@percent }
\citation{imageNet_dataset}
\citation{resnet0}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Convolución como una operación lineal}{23}{subsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Redes Neuronales Convolucionales}{23}{section.2.2}\protected@file@percent }
\newlabel{section_cnn}{{2.2}{23}{Redes Neuronales Convolucionales}{section.2.2}{}}
\newlabel{clasification}{{2.9}{23}{Redes Neuronales Convolucionales}{equation.2.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Descenso del Gradiente Estocástico}{24}{subsection.2.2.1}\protected@file@percent }
\citation{adagrad}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \leavevmode {\color  {red}Seleccionar su propia imagen}}}{25}{figure.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Optimizadores}{25}{subsection.2.2.2}\protected@file@percent }
\newlabel{subsection:optimizadores}{{2.2.2}{25}{Optimizadores}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{AdaGrad}{25}{section*.6}\protected@file@percent }
\citation{adam}
\@writefile{toc}{\contentsline {subsubsection}{RMSProp}{26}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adam}{26}{section*.8}\protected@file@percent }
\citation{batchNormalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Propagación hacia atrás}{27}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Normalización por Lotes}{27}{subsection.2.2.4}\protected@file@percent }
\citation{image:cnn}
\citation{image:cnn}
\citation{alexnet}
\citation{zfnet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Ejemplo de arquitectura de una Red Neuronal Convolucional. En la figura se aprecia la imagen de entrada, las capas convolucionales y la FCN que determina la clase \cite  {image:cnn}}}{28}{figure.2.7}\protected@file@percent }
\newlabel{cnn_example_img}{{2.7}{28}{Ejemplo de arquitectura de una Red Neuronal Convolucional. En la figura se aprecia la imagen de entrada, las capas convolucionales y la FCN que determina la clase \cite {image:cnn}}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Desvanecimiento del gradiente}{28}{subsection.2.2.5}\protected@file@percent }
\citation{resnet0}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}CNNs en el Estado del Arte}{29}{subsection.2.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Línea de tiempo de las Redes en el Estado del Arte}}{29}{figure.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}ResNet}{29}{section.2.3}\protected@file@percent }
\newlabel{resnet_section}{{2.3}{29}{ResNet}{section.2.3}{}}
\newlabel{resnet_equation}{{2.21}{30}{}{equation.2.3.21}{}}
\newlabel{resnet_equation_modified}{{2.22}{30}{ResNet}{equation.2.3.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Diagrama de un bloque de ResNnet.}}{30}{figure.2.9}\protected@file@percent }
\citation{DBLP:journals/corr/abs-1806-03751}
\citation{pytorch_library}
\newlabel{resnet_block_function}{{2.24}{31}{ResNet}{equation.2.3.24}{}}
\citation{weight_initialization}
\citation{deeplearningbook}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Diagrama de una ResNet-50}}{32}{figure.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Inicialización de parámetros}{32}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inicialización con Ceros}{32}{section*.9}\protected@file@percent }
\citation{glorot_initialization}
\citation{kaiming}
\citation{CNNdefinition}
\@writefile{toc}{\contentsline {subsubsection}{Inicialización Aleatoria}{33}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inicialización de Glorot}{33}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inicialización con Kaiming}{33}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Estabilidad en las redes Neuronales}{33}{subsection.2.3.2}\protected@file@percent }
\newlabel{resnet_stability_introduction}{{2.3.2}{33}{Estabilidad en las redes Neuronales}{subsection.2.3.2}{}}
\citation{stable_resnets}
\citation{sauer,Ascher}
\citation{ode_history}
\citation{geometry_and_odes}
\citation{classical_mechanics}
\citation{electromagnetism_and_odes}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Ecuaciones diferenciales}{35}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{odes_chapter}{{3}{35}{Ecuaciones diferenciales}{chapter.3}{}}
\citation{sauer}
\citation{richard-Bellman,Hyers-Ulam}
\citation{ascher-book}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problemas de valor inicial}{36}{section.3.1}\protected@file@percent }
\newlabel{ode}{{3.3}{36}{Problemas de valor inicial}{equation.3.1.3}{}}
\newlabel{initial_value}{{3.4}{36}{Problemas de valor inicial}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Existencia, unicidad y continuidad de soluciones}{36}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Estabilidad}{36}{section.3.2}\protected@file@percent }
\newlabel{ode_stability}{{3.2}{36}{Estabilidad}{section.3.2}{}}
\newlabel{stable-ode}{{3.7}{37}{}{equation.3.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Problema Lineal}{37}{subsection.3.2.1}\protected@file@percent }
\newlabel{hom_ivp}{{3.12}{37}{Problema Lineal}{equation.3.2.12}{}}
\newlabel{hom_init}{{3.13}{37}{Problema Lineal}{equation.3.2.13}{}}
\newlabel{lineal_ivp}{{3.5}{38}{}{theorem.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}IVP no lineales (El caso general)}{38}{subsection.3.2.2}\protected@file@percent }
\newlabel{no_lineal_ivp}{{3.2.2}{38}{IVP no lineales (El caso general)}{subsection.3.2.2}{}}
\newlabel{original_ode}{{3.16}{38}{IVP no lineales (El caso general)}{equation.3.2.16}{}}
\newlabel{taylor_for_ivp}{{3.18}{38}{IVP no lineales (El caso general)}{equation.3.2.18}{}}
\citation{book:110336}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Métodos de Runge-Kutta}{39}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Método de Euler}{39}{subsection.3.3.1}\protected@file@percent }
\newlabel{section:euler_method}{{3.3.1}{39}{Método de Euler}{subsection.3.3.1}{}}
\newlabel{taylor}{{3.22}{39}{Método de Euler}{equation.3.3.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Error de truncamiento local}{40}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Error de truncamiento global}{40}{section*.14}\protected@file@percent }
\citation{sauer}
\newlabel{bound_global_error}{{3.30}{41}{Error de truncamiento global}{equation.3.3.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Método de Euler modificado}{41}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Método general}{41}{subsection.3.3.3}\protected@file@percent }
\citation{symplecticRK}
\citation{Sanz-Serna1988-mk,Lasagni1988-ov}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Runge Kutta de orden 4 (RK4)}{42}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Métodos de Runge Kutta simplécticos}{42}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Métodos implícitos}{43}{subsection.3.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Euler hacia atrás}{43}{section*.15}\protected@file@percent }
\newlabel{backward_euler}{{3.38}{43}{Euler hacia atrás}{equation.3.3.38}{}}
\citation{stable_resnets}
\citation{suresh}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Teoría de control óptimo}{45}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch2system}{{4.1}{45}{Teoría de control óptimo}{equation.4.0.1}{}}
\newlabel{ch2controlsystem}{{4.2}{45}{Teoría de control óptimo}{equation.4.0.2}{}}
\citation{Engineerin_Economy}
\citation{suresh}
\citation{optimal_principle}
\newlabel{ch2umax}{{4.4}{46}{}{equation.4.0.4}{}}
\newlabel{ch2dynamicCondition}{{4.5}{46}{}{equation.4.0.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}La ecuación de Hamilton-Jacobi-Bellman}{46}{section.4.1}\protected@file@percent }
\newlabel{value_function}{{4.6}{46}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.6}{}}
\newlabel{step_one}{{4.7}{47}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.7}{}}
\newlabel{step_two}{{4.8}{47}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.8}{}}
\newlabel{2.11}{{4.9}{47}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.9}{}}
\newlabel{pre_hamiltonian}{{4.13}{47}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.13}{}}
\newlabel{marginal_eq}{{4.15}{47}{}{equation.4.1.15}{}}
\newlabel{hamiltonian_def_eq}{{4.16}{48}{}{equation.4.1.16}{}}
\newlabel{HJB}{{4.18}{48}{La ecuación de Hamilton-Jacobi-Bellman}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Ecuación adjunta}{48}{section.4.2}\protected@file@percent }
\newlabel{almost_adjoint_eq}{{4.23}{48}{Ecuación adjunta}{equation.4.2.23}{}}
\newlabel{4.26}{{4.26}{49}{Ecuación adjunta}{equation.4.2.26}{}}
\newlabel{4.27}{{4.27}{49}{Ecuación adjunta}{equation.4.2.27}{}}
\newlabel{4.28}{{4.28}{49}{Ecuación adjunta}{equation.4.2.28}{}}
\newlabel{4.29}{{4.29}{49}{Ecuación adjunta}{equation.4.2.29}{}}
\newlabel{4.34}{{4.34}{49}{Ecuación adjunta}{equation.4.2.34}{}}
\citation{numerical_ode_and_architectures,DBLP:journals/corr/LiaoP16}
\citation{odes_and_ml_survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Relación entre las ecuaciones diferenciales y las ResNets}{51}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{odes_and_resnets}{{5}{51}{Relación entre las ecuaciones diferenciales y las ResNets}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Control Óptimo para DL}{51}{section.5.1}\protected@file@percent }
\citation{optimal_control_approach,mean_field_optimal_control}
\citation{PMP_for_DL}
\citation{DL_as_OCP}
\citation{DBLP:journals/corr/LiaoP16,dnns_motivated_by_pdes,numerical_ode_and_architectures}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}DNNs como discretización de ODEs}{52}{section.5.2}\protected@file@percent }
\newlabel{resnet_equation_again}{{5.3}{52}{DNNs como discretización de ODEs}{equation.5.2.3}{}}
\newlabel{discrete_resnet}{{5.4}{52}{DNNs como discretización de ODEs}{equation.5.2.4}{}}
\newlabel{continuos_resnet}{{5.5}{52}{DNNs como discretización de ODEs}{equation.5.2.5}{}}
\newlabel{example_of_f}{{5.6}{52}{DNNs como discretización de ODEs}{equation.5.2.6}{}}
\citation{stable_resnets}
\newlabel{evident_discretization}{{5.7}{53}{DNNs como discretización de ODEs}{equation.5.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Estabilidad de Redes por medio de ODEs}{53}{subsection.5.2.1}\protected@file@percent }
\newlabel{stability_criteria}{{5.9}{53}{Estabilidad de Redes por medio de ODEs}{equation.5.2.9}{}}
\citation{stable_resnets}
\citation{polynet}
\citation{numerical_ode_and_architectures}
\newlabel{stability_criteria2}{{5.16}{54}{Estabilidad de Redes por medio de ODEs}{equation.5.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}PolyNet}{54}{subsection.5.2.2}\protected@file@percent }
\citation{fractalNet}
\newlabel{inverse_identity}{{5.18}{55}{PolyNet}{equation.5.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}FractalNet}{55}{subsection.5.2.3}\protected@file@percent }
\citation{fractalNet}
\citation{fractalNet}
\citation{fractalNet}
\citation{numerical_ode_and_architectures}
\citation{reversible_nets}
\citation{reversible_definitions}
\newlabel{fractal_paper_eq}{{5.22}{56}{FractalNet}{equation.5.2.22}{}}
\newlabel{fractalnet_eq}{{5.23}{56}{FractalNet}{equation.5.2.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Representción gráfica de un bloque de la FractalNet extraída del artículo original \cite  {fractalNet}.}}{56}{figure.5.1}\protected@file@percent }
\citation{reversible_nets}
\citation{computer_methods}
\citation{imex_method}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Midpoint Network}{57}{subsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}IMEXnet}{57}{subsection.5.2.5}\protected@file@percent }
\citation{INEX-NET}
\citation{INEX-NET}
\citation{chang2019antisymmetricrnn,stable_resnets}
\citation{reversible_nets}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Otros Avances}{58}{subsection.5.2.6}\protected@file@percent }
\citation{PDE_nets}
\citation{numerical_ode_and_architectures}
\citation{stable_resnets,reversible_nets}
\citation{polen}
\citation{polen}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experimentos y Resultados}{61}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{experimentos}{{6}{61}{Experimentos y Resultados}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Descripción del problema}{61}{section.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Imagen extraída de la página oficial del Pollen Challenge \cite  {polen}}}{62}{figure.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Métricas para clasificación Binaria}{62}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Matriz de confusión}{63}{subsection.6.2.1}\protected@file@percent }
\newlabel{pos_neg}{{6.1}{63}{}{theorem.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \leavevmode {\color  {red}Conseguir imagen propia}}}{64}{figure.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Exactitud}{64}{subsection.6.2.2}\protected@file@percent }
\citation{f1score}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Sensibilidad y Especificidad}{65}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Precisión}{65}{subsection.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}\leavevmode {\color  {red}F1-score}}{65}{subsection.6.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Métricas para clasificación multiclase}{66}{section.6.3}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{papers}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \leavevmode {\color  {red}Conseguir imagen propia}}}{67}{figure.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Métricas Macro}{68}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Métricas Ponderadas}{68}{subsection.6.3.2}\protected@file@percent }
\bibcite{computer_vision}{1}
\bibcite{Mitchell}{2}
\bibcite{transfer}{3}
\bibcite{grafica}{4}
\bibcite{VAE}{5}
\bibcite{SMOTE}{6}
\bibcite{MSMOTE}{7}
\bibcite{firstCnn}{8}
\bibcite{lecunCnn}{9}
\bibcite{alexnet}{10}
\bibcite{CNNdefinition}{11}
\bibcite{deeplearningbook}{12}
\bibcite{convolution_for_quaternion}{13}
\bibcite{convolution_invariance}{14}
\bibcite{padding}{15}
\bibcite{type_of_paddings}{16}
\bibcite{pooling_analysis}{17}
\bibcite{imageNet_dataset}{18}
\bibcite{resnet0}{19}
\bibcite{adagrad}{20}
\bibcite{adam}{21}
\bibcite{batchNormalization}{22}
\bibcite{image:cnn}{23}
\bibcite{zfnet}{24}
\bibcite{DBLP:journals/corr/abs-1806-03751}{25}
\bibcite{pytorch_library}{26}
\bibcite{weight_initialization}{27}
\bibcite{glorot_initialization}{28}
\bibcite{kaiming}{29}
\bibcite{stable_resnets}{30}
\bibcite{sauer}{31}
\bibcite{Ascher}{32}
\bibcite{ode_history}{33}
\bibcite{geometry_and_odes}{34}
\bibcite{classical_mechanics}{35}
\bibcite{electromagnetism_and_odes}{36}
\bibcite{richard-Bellman}{37}
\bibcite{Hyers-Ulam}{38}
\bibcite{ascher-book}{39}
\bibcite{book:110336}{40}
\bibcite{symplecticRK}{41}
\bibcite{Sanz-Serna1988-mk}{42}
\bibcite{Lasagni1988-ov}{43}
\bibcite{suresh}{44}
\bibcite{Engineerin_Economy}{45}
\bibcite{optimal_principle}{46}
\bibcite{numerical_ode_and_architectures}{47}
\bibcite{DBLP:journals/corr/LiaoP16}{48}
\bibcite{odes_and_ml_survey}{49}
\bibcite{optimal_control_approach}{50}
\bibcite{mean_field_optimal_control}{51}
\bibcite{PMP_for_DL}{52}
\bibcite{DL_as_OCP}{53}
\bibcite{dnns_motivated_by_pdes}{54}
\bibcite{polynet}{55}
\bibcite{fractalNet}{56}
\bibcite{reversible_nets}{57}
\bibcite{reversible_definitions}{58}
\bibcite{computer_methods}{59}
\bibcite{imex_method}{60}
\bibcite{INEX-NET}{61}
\bibcite{chang2019antisymmetricrnn}{62}
\bibcite{PDE_nets}{63}
\bibcite{polen}{64}
\bibcite{f1score}{65}
