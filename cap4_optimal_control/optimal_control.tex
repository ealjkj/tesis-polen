%%%%%%%%%%%%%%%%%%%%%%%%% CHAPTER 1%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Teoría de control óptimo}
    
Las ResNets guardan cierta relación con las Ecuaciones Diferenciales \cite{stable_resnets}. En el Capítulo \ref{odes_and_resnets} se estudia la conexión entre las DNNs y las ODEs. Debido a esto y a que el problema de aprendizaje implica optimizar una función, el problema es equivalente a encontrar un \textsl{control óptimo}   

A continuación, se desarrollarán algunos resultados de Teoría de control  óptimo con base en \cite{suresh}.  

Cuando se habla de sistemas, se hace referencia a un proceso que cambia de estado conforme pasa el tiempo. En este caso considérese un sistema dinámico definido en el intervalo $[0, T]$ como
\begin{equation}
    \label{ch2system}
    \dot x =f(x(t),t), \quad x(0) = x_0,  
\end{equation}
donde $x(t)$, conocida como la \textsl{variable de estado} puede representar la producción de alimento en un tiempo $t$, el dinero recaudado en un tiempo $t$, o en general el estado de un proceso en un tiempo $t$. En logística y física, existen variables que se pueden controlar, como la cantidad de publicidad, o la reinversión de capital cada cierto momento, con lo que es posible extender el sistema  (\ref{ch2system}) al siguiente:
\begin{equation}
    \label{ch2controlsystem}
    \dot x = f(x(t),u(t),t), \quad x(0) = x_0,
\end{equation}
donde la variable $u(t)$ es denominada \textsl{variable de control}. Conociendo la variable de control, es posible determinar la solución para $x$ en el sistema (\ref{ch2controlsystem}). El problema de control óptimo reside en maximizar la \textsl{función objetivo}, definida como
\begin{equation}
    J = \int_0^T F(x(t), u(t), t)dt + S[x(T), T].
\end{equation}
 La función $S$ determina el \textsl{valor de rescate} en el estado final $x(T)$ en el tiempo $T$, y obtiene pues en el área de economía es \textsl{el valor de un activo, una vez que ha concluido su vida útil} \cite{Engineerin_Economy}. Se denotará al conjunto de valores posibles de $u(t)$ como $\Omega(t)$.

\begin{definition}
    El \textsl{problema de control óptimo} es encontrar un valor admisible $u^*$ tal que 
    \begin{equation}
        \label{ch2umax}
        u^* = \max_{u(t)\in \Omega(t)}\left\{\int_0^T F(x, u, t)dt + S(x(T), T)\right\}
    \end{equation}
    con la condición 
    \begin{equation}
        \label{ch2dynamicCondition}
        \dot x = f(x,u,t), \quad x(0) = 0.
    \end{equation}
    La trayectoria óptima denotada como $x^*$ es la trayectoria que se obtiene cuando $u = u^*$.
\end{definition}

%Ecuación de Hamilton Jacobi Bellman
\section{La ecuación de Hamilton-Jacobi-Bellman}
Para estudiar la Ecuación de Hamilton-Jacobi-Bellman, es necesario el principio de optimalidad, el cuál establece que para obtener un camino óptimo de 0 a $T$, es necesario que para toda $t$, el camino de $t$ a $T$ sea óptimo también. Es importante remarcar que la siguiente prueba se encuentra en \cite{suresh} y se especifica que no tiene el propósito de ser rigurosa.

Sea $V: \R^n \times \R \to \R$ una función definida por
\begin{equation}
    \label{value_function}
    V(x,t) = \max_{u(s)\in \Omega(s)}\left\{\int_t^T F(x(s), u(s), s)ds + S(x(T), T)\right\}
\end{equation}
donde $s \geq t$.
Sea $\delta t$ un diminuto incremento en el tiempo, nótese que 
\begin{align*}
    V(x,t) - V(x(t + \delta t), t + \delta t) &= \max_{u(s)\in \Omega(s)}\left\{\int_t^T F(x(s), u(s), s)ds + S(x(T), T)\right\}  \\
    & \quad - \max_{u(s)\in \Omega(s)}\left\{\int_{t+ \delta}^T F(x(s), u(s), s)ds + S(x(T), T)\right\} \\
\end{align*}
Lo cuál, gracias al principio de optimalidad \cite{optimal_principle} se tiene que 
\begin{equation}
    \label{step_one}
   V(x(t), t) - V(x(t + \delta t), t + \delta t) = \max_{u(\tau)\in \Omega(\tau)} \left\{ \int_{t}^{t+ \delta t} F(x(\tau), u(\tau), \tau)\right\},
\end{equation} 
y por consiguiente
\begin{equation}
    \label{step_two}
    V(x(t), t) = \max_{u(\tau)\in \Omega(\tau)} \left\{ \int_{t}^{t+ \delta t} F(x(\tau), u(\tau), \tau) + V(x(t + \delta t), t + \delta t)\right\}.
 \end{equation} 
 Debido a que $F$ es una función continua, la integral en (\ref{step_two}) es igual a $F(x,u,t)\delta t$. Por consiguiente:
 \begin{equation}
    \label{2.11}
     V(x,t) = \max_{u\in \Omega(t)} F(x,u,t)\delta t + V[x(t+\delta t), t + \delta t] + \mathcal O(\delta t)
 \end{equation} 
 Asumiendo que la función $V$ es continuamente diferenciable en ambas entradas es posible hacer uso de la expansión de Taylor de $V$ en $\delta t$:
 \begin{equation}
    V[x(t + \delta t), t + \delta t] = V(x,t) + [V_x(x,t)\dot x + V_t(x,t)]\delta t + \mathcal O(\delta t)
 \end{equation}
 Sustituyendo (\ref{ch2controlsystem}) en (\ref{2.11}) se obtiene que 
 \begin{equation}
     V(x,t) = \max_{u\in \Omega(t)} \{F(x,u,t)\delta t + V(x,t) + V_x(x,t)f(x,u,t)\delta t + V_t(x,t)\delta t\} + \mathcal O(\delta t)
 \end{equation}
Cancelando $V(x,t)$ de ambos lados y dividiendo entre $\delta t$ se deduce que
\begin{equation}
    0 = \max_{u\in \Omega(t)} \{F(x,u,t) + V_x(x,t)f(x,u,t) + V_t(x,t)\} + \frac{\mathcal O(\delta t)}{\delta t},
\end{equation}
con lo que si $\delta t \to 0$ se tiene que 
\begin{equation}
    \label{pre_hamiltonian}
    0 = \max_{u\in \Omega(t)} \{F(x,u,t) + V_x(x,t)f(x,u,t) + V_t(x,t)\}
\end{equation}
dónde si se evalúa $t=T$ en $V$ se obtiene que 
\begin{equation}
    V(x,T) = S(x,T).
\end{equation}
Cuando la función $V$ es aplicada en el estado óptimo $x^*$ se obtiene un nuevo vetor
\begin{definition}(Vector de retorno marginal)
    Sea $V$ la función definida en (\ref{value_function}), el vector de retorno marginal $\lambda(t)$ se define como
    \begin{equation}
        \label{marginal_eq}
        \lambda(t) = V_x(x^*(t),t):=V_x(x,t)|_{x=x^*}
    \end{equation}
\end{definition}
Lo cuál será util para definir el concepto de Hamiltoniano.
\begin{definition}
    Considérese el sistema (\ref{ch2umax})-(\ref{ch2dynamicCondition}). Se denota el hamiltoniano $H: \R^n\times \R^m \times \R^n \times \R \to \R$ del sistema como 
    \begin{equation}
        \label{hamiltonian_def_eq}
        H(x,u,\lambda, t) = F(x,u,t) + \lambda f(x,u,t).
    \end{equation}
\end{definition}
La ecuación (\ref{pre_hamiltonian}) puede reescribirse como
\begin{equation}
    \max_{u\in \Omega(t)}[H(x,u,V_x,t) + V_t] = 0
\end{equation}
la cuál se conoce como \textsl{Ecuación de Hamilton-Jacobi-Bellman}(HJB). Más aún, ya que $V_t$ no depende de $u$ es posible retirar $V_t$ del operador $\max$.
\begin{equation}
    \label{HJB}
    \max_{u\in \Omega(t)}[H(x,u,V_x,t)] + V_t = 0.
\end{equation}
Gracias a la definición del control óptimo y de $\lambda(t)$, el operador $u^*$ maximiza la ecuación (\ref{HJB}), por tanto sea $u\in \Omega(t)$, se tiene que
\begin{equation}
    H[x^*(t), u^*(t), \lambda(t), t] + V_t(x^*(t),t ) \geq H[x^*(t), u(t), \lambda(t), t] + V_t(x^*(t),t )
\end{equation}
\begin{equation}
    \Rightarrow H[x^*(t), u^*(t), \lambda(t), t]  \geq H[x^*(t), u(t), \lambda(t), t].
\end{equation}

\section{Ecuación adjunta}
Nótese que para encontrar el control óptimo, la ecuación (\ref{HJB}) se maximiza con $x = x^*$ y $u = u^*$. Consideremos entonces pequeñas perturbaciones en el camino óptimo. Sea $x(t) = x^*(t) + \delta x(t)$ y sea $t\in [0,T]$, podemos escribir (\ref{HJB}) como:
\begin{align}
    0 &= H[x^*(t), u^*(t), V_x(x^*(t), t), t] + V_x(x^*(t),  t) \\
     &\geq H[x(t), u^*(t), V_x(x(t), t), t] + V_x(x(t), t).
\end{align}
Lo cuál significa que $x^*(t)$ es un máximo local, de modo que la derivada con respecto a $x$, debe ser 0.
\begin{equation}
    \label{almost_adjoint_eq}
    H_x[x^*(t), u^*(t), V_x(x^*(t), t), t] + V_{tx}(x^*(t),t) = 0,
\end{equation} 
Asúmase además que $V$ es doblemente diferenciable en todos sus argumentos. Por otro lado, con $H = F+ V_xf$, podemos derivar con respecto de $x$
\begin{align}
    H =& F + V_xf \\
    \Rightarrow H_x &= \frac{\partial(F + V_xf)}{\partial x} \\
    \label{4.26}  \Rightarrow H_x &= F_x + \frac{\partial(V_xf)}{\partial x} \\
    \label{4.27}\Rightarrow H_x &= F_x + V_xF_x  + f^TV_{xx} \\
    \label{4.28}\Rightarrow H_x &= F_x + V_xF_x  + (V_{xx}f)^T 
\end{align}
Nótese que de (\ref{4.26}) a (\ref{4.27}) se hizo uso de la derivada de un producto y para (\ref{4.28}) que $V_{xx}$ es una matriz simétrica. Sustituyendo esto en (\ref{almost_adjoint_eq}) obtenemos:
\begin{equation}
    \label{4.29}
    H_x = F_x + V_xF_x  + (V_{xx}f)^T + V_{tx} = 0.
\end{equation}
Tómese ahora la derivada de $V_x$ con respecto al tiempo
\begin{align}
    \frac{dV_x}{dt} &= \left(\frac{dV_{x_1}}{dt}, \frac{dV_{x_2}}{dt}, \cdots \frac{dV_{x_n}}{dt} \right)  \\
    &= (V_{x_1x}\dot x + V_{x_1t}, V_{x_2x}\dot x + V_{x_2t},  \cdots, V_{x_nx}) \\
    &= (\sum_{i=1}^n V_{x_1x_i} \dot x_i, \cdots, \sum_{i=1}^n V_{x_nx_i} \dot x_i ) + (V_x)_t \\
    & = (V_{xx} \dot x)^T + V_{xt} \\
    \label{4.34} & = (V_{xx}  f)^T + V_{tx} 
\end{align}
Combinando (\ref{4.29}) con (\ref{4.34}) se obtiene que 
\begin{equation}
    F_x + V_xF_x + \frac{dV_x}{dt} = 0 
\end{equation}
y ya que $\lambda$ fue definida como $V_x$, se tiene lo siguiente:
\begin{equation}
    \dot \lambda = -F_x - \lambda f_x.
\end{equation}
Sin embargo, el lado derecho de esta ecuación, es igual a $H_x$ debido a la definición del hamiltoniano $H$. Con lo que se puede reescribir como 
\begin{equation}
    \dot \lambda = - H_x. 
\end{equation}

